{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has been trained inside img_align_celeba which is a big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load attribute data\n",
    "attr_df = pd.read_csv('list_attr_celeba.csv')\n",
    "attr_df['image_id'] = attr_df['image_id'].astype(str)\n",
    "\n",
    "# Directory containing images\n",
    "img_dir = 'img_align_celeba'  # Replace with your image directory\n",
    "\n",
    "# Define function to load and preprocess images\n",
    "def load_and_preprocess_images(attr_df, img_dir, target_size=(224, 224)):\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx, row in attr_df.iterrows():\n",
    "        img_id = row['image_id']\n",
    "        img_path = f'{img_dir}/{img_id}'\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue  # Skip if image cannot be read\n",
    "        \n",
    "        # Resize image\n",
    "        img = cv2.resize(img, target_size)\n",
    "        \n",
    "        # Normalize pixel values (assuming RGB images)\n",
    "        img = img / 255.0\n",
    "        \n",
    "        X.append(img)\n",
    "        \n",
    "        # Extract attribute labels\n",
    "        attr_labels = row.iloc[1:].values.astype(int)  # Skip 'image_id' column\n",
    "        attr_labels[attr_labels == -1] = 0  # Replace -1 with 0\n",
    "        y.append(attr_labels)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and preprocess images\n",
    "X, y = load_and_preprocess_images(attr_df[:1000], img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.8530 - loss: 0.5372 - val_accuracy: 0.9062 - val_loss: 0.3139\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6436 - loss: 0.9125 - val_accuracy: 0.6812 - val_loss: 0.6323\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.5080 - loss: 0.8305 - val_accuracy: 0.4375 - val_loss: 0.7033\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6725 - loss: 0.7047 - val_accuracy: 0.8000 - val_loss: 0.5182\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.9699 - loss: 0.2221 - val_accuracy: 0.9812 - val_loss: 0.0941\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.8614 - loss: 0.5356 - val_accuracy: 0.8250 - val_loss: 0.5121\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.7234 - loss: 0.6950 - val_accuracy: 0.6562 - val_loss: 0.6457\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6207 - loss: 0.7734 - val_accuracy: 0.7875 - val_loss: 0.5260\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6440 - loss: 0.6763 - val_accuracy: 0.7375 - val_loss: 0.5656\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.8287 - loss: 0.6095 - val_accuracy: 0.8625 - val_loss: 0.3981\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.8048 - loss: 0.4595 - val_accuracy: 0.9750 - val_loss: 0.1945\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.7754 - loss: 0.6465 - val_accuracy: 0.8000 - val_loss: 0.5327\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.8748 - loss: 0.5758 - val_accuracy: 0.8375 - val_loss: 0.4581\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.7742 - loss: 0.5173 - val_accuracy: 0.9187 - val_loss: 0.3003\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.7992 - loss: 0.3895 - val_accuracy: 0.9625 - val_loss: 0.1609\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.9222 - loss: 0.3802 - val_accuracy: 0.9375 - val_loss: 0.2494\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.7691 - loss: 0.4497 - val_accuracy: 0.9375 - val_loss: 0.2420\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.9348 - loss: 0.3618 - val_accuracy: 0.9750 - val_loss: 0.1359\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.5484 - loss: 0.9295 - val_accuracy: 0.5562 - val_loss: 0.8632\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.5336 - loss: 0.8451 - val_accuracy: 0.4938 - val_loss: 0.7387\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.4585 - loss: 0.9397 - val_accuracy: 0.6250 - val_loss: 0.6726\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.4524 - loss: 0.9298 - val_accuracy: 0.5063 - val_loss: 0.7192\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.8060 - loss: 0.3653 - val_accuracy: 0.9750 - val_loss: 0.1948\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.7643 - loss: 0.5217 - val_accuracy: 0.8875 - val_loss: 0.3508\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.6980 - loss: 0.6855 - val_accuracy: 0.8375 - val_loss: 0.5017\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6093 - loss: 0.6938 - val_accuracy: 0.6938 - val_loss: 0.6352\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.7956 - loss: 0.5019 - val_accuracy: 0.9688 - val_loss: 0.1437\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6505 - loss: 0.7169 - val_accuracy: 0.6812 - val_loss: 0.6269\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9428 - loss: 0.3637 - val_accuracy: 0.8938 - val_loss: 0.4747\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.8092 - loss: 0.5074 - val_accuracy: 0.9125 - val_loss: 0.3985\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.9419 - loss: 0.2766 - val_accuracy: 0.9688 - val_loss: 0.1411\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.4469 - loss: 0.9672 - val_accuracy: 0.5500 - val_loss: 0.6904\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.6495 - loss: 0.6789 - val_accuracy: 0.8125 - val_loss: 0.5656\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6355 - loss: 0.7742 - val_accuracy: 0.6875 - val_loss: 0.6215\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.6151 - loss: 0.7440 - val_accuracy: 0.7625 - val_loss: 0.5503\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.8072 - loss: 0.5792 - val_accuracy: 0.9563 - val_loss: 0.3781\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.5062 - loss: 0.9231 - val_accuracy: 0.4875 - val_loss: 0.6903\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.8615 - loss: 0.5300 - val_accuracy: 0.8500 - val_loss: 0.4296\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.7679 - loss: 0.5443 - val_accuracy: 0.9500 - val_loss: 0.1986\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.6552 - loss: 0.7479 - val_accuracy: 0.7875 - val_loss: 0.5210\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze ResNet50 layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Function to create model for a single attribute\n",
    "def create_model():\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "    \n",
    "    model = models.Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create 40 models, one for each attribute\n",
    "models_dict = {}\n",
    "for col in attr_df.columns[1:]:  # Iterate over attribute columns\n",
    "    model = create_model()\n",
    "    models_dict[col] = model\n",
    "\n",
    "# Train each model separately\n",
    "histories = {}\n",
    "for col, model in models_dict.items():\n",
    "    y_train_col = y_train[:, attr_df.columns[1:].tolist().index(col)]\n",
    "    y_val_col = y_val[:, attr_df.columns[1:].tolist().index(col)]\n",
    "    \n",
    "    history = model.fit(X_train, y_train_col,\n",
    "                        validation_data=(X_val, y_val_col),\n",
    "                        epochs=1,\n",
    "                        batch_size=32)\n",
    "    \n",
    "    histories[col] = history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume models_dict contains all the trained models for each attribute\n",
    "for col, model in models_dict.items():\n",
    "    # Save each model to a file named after the corresponding column\n",
    "    model.save(f'{col}_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 434 variables whereas the saved optimizer has 10 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Directory containing the saved models\n",
    "models_dir = os.getcwd()  # Replace with your directory containing the models\n",
    "\n",
    "# Dictionary to store the loaded models\n",
    "loaded_models = {}\n",
    "cnt = 0\n",
    "# Iterate through the files in the directory\n",
    "for filename in os.listdir(models_dir):\n",
    "    if filename.endswith('.keras'):\n",
    "        cnt = cnt+1\n",
    "        # Extract the model name from the filename (without the extension)\n",
    "        model_name = os.path.splitext(filename)[0]\n",
    "        # Load the model\n",
    "        model_path = os.path.join(models_dir, filename)\n",
    "        loaded_models[model_name] = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        if(cnt == 7):\n",
    "            break \n",
    "\n",
    "# Now, loaded_models contains all the models loaded from the directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Predictions for Sample Image: [0.07799012, 0.24369387, 0.44508985, 0.2881811, 0.009382524, 0.30842838, 0.38269156]\n"
     ]
    }
   ],
   "source": [
    "# Example: Predict attributes for a single image using loaded models\n",
    "img_dir = \"img_align_celeba\"\n",
    "sample_img_path = f'{img_dir}/{attr_df.iloc[0][\"image_id\"]}'\n",
    "sample_img = load_img(sample_img_path, target_size=(224, 224))\n",
    "sample_img = img_to_array(sample_img)\n",
    "sample_img = np.expand_dims(sample_img / 255.0, axis=0)  # Normalize and add batch dimension\n",
    "\n",
    "sample_preds = []\n",
    "\n",
    "for col, loaded_model in loaded_models.items():\n",
    "    sample_pred = loaded_model.predict(sample_img)\n",
    "    sample_preds.append(sample_pred[0][0])\n",
    "\n",
    "print(\"Predictions for Sample Image:\", sample_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caching the list of root modules, please wait!\n",
      "(This will only be done once - type '%rehashx' to reset cache!)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_preds = np.array(sample_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1]\n",
      "5_o_Clock_Shadow: No\n",
      "Arched_Eyebrows: No\n",
      "Attractive: No\n",
      "Bags_Under_Eyes: No\n",
      "Bald: No\n",
      "Bangs: No\n",
      "Big_Lips: No\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "binary_pred = (sample_preds >= threshold).astype(int) * 2 - 1\n",
    "\n",
    "# Attribute descriptions\n",
    "attribute_descriptions = [\n",
    "    \"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\", \"Big_Lips\",\n",
    "]\n",
    "# print(binary_pred)\n",
    "# Describe the attributes present in the sample_pred\n",
    "descriptions = []\n",
    "for i, pred in enumerate(binary_pred):\n",
    "    if pred == 1:\n",
    "        descriptions.append(f\"{attribute_descriptions[i]}: Yes\")\n",
    "    else:\n",
    "        descriptions.append(f\"{attribute_descriptions[i]}: No\")\n",
    "\n",
    "# Print the descriptions\n",
    "for desc in descriptions:\n",
    "    print(desc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
